{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Corpus Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(self, doc):\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    content = re.sub('[^A-Za-z]+', ' ', doc)\n",
    "    content = content.lower().split()\n",
    "    content = ' '.join([word for word in content if word not in stop_words and len(word) > 1])\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens(self, file):\n",
    "    input_file = open(file, \"r\", encoding=\"utf8\")\n",
    "    all_words = list()\n",
    "    vocab_tokens = list()\n",
    "\n",
    "    for line in input_file:\n",
    "        line.rstrip()\n",
    "        words = line.split()\n",
    "        all_words.extend(words)\n",
    "\n",
    "    for word in allWords:\n",
    "        word = re.sub(r'\\b[^\\W\\d_]+\\b', '', word)\n",
    "        word = word.lower().strip()\n",
    "        if not word.isdigit():\n",
    "            if word not in vocab_tokens:\n",
    "                vocab_tokens.append(word)\n",
    "\n",
    "    return vocab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common(self, content):\n",
    "    content = ' '.join(content)\n",
    "    lines = content.lower().splitlines()\n",
    "\n",
    "    prep = [preprocess_document(x) for x in lines]\n",
    "\n",
    "    words = [x for sublist in prep for x in sublist]\n",
    "\n",
    "    count = Counter(words)\n",
    "    \n",
    "    return count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corpus_statistics(self, file, most_common, preprocessed= False):\n",
    "    tokens_occuring_once = list()\n",
    "    tokens_occuring_1000_plus = list()\n",
    "    # ref. http://stackoverflow.com/questions/25985299/create-python-dictionary-from-text-file-and-retrieve-count-of-each-word\n",
    "    with open(file, \"r\", encoding=\"utf8\") as f:\n",
    "        c = collections.Counter(\n",
    "            word.lower()\n",
    "            for line in f\n",
    "            for word in re.findall(r'\\b[^\\W\\d_]+\\b', line))\n",
    "    collection_len = len(c)\n",
    "\n",
    "    if preprocessed:\n",
    "        vocab_size = len(self.get_unique_tokens_preprocessed(file))\n",
    "    else:\n",
    "        vocab_size = len(self.get_unique_tokens(file))\n",
    "\n",
    "    print (\"Total word occurences: %d\" % sum(c.values()))\n",
    "    print (\"Vocabulary size: %d\" % vocab_size)\n",
    "\n",
    "    print ('Most common words:')\n",
    "    for letter, count in c.most_common(most_common):\n",
    "        print ('%s: %7d' % (letter, count))\n",
    "\n",
    "    for letter, count in c.most_common(collection_len):\n",
    "        if count > 1000:\n",
    "            tokens_occuring_1000_plus.append(letter)\n",
    "        if count == 1:\n",
    "            tokens_occuring_once.append(letter)\n",
    "\n",
    "    print (\"There are %d words occuring > 1000 times\" % len(tokens_occuring_1000_plus))\n",
    "    print (\"There are %d words occuring once\" % len(tokens_occuring_once))\n",
    "\n",
    "    return tokens_occuring_1000_plus, tokens_occuring_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_docs(self, file, keyword, keep_rare_words = True):\n",
    "    data = self.parse_docs(file.readlines(), keyword, keep_rare_words)\n",
    "    return data\n",
    "\n",
    "def parse_docs(self, lines, keyword, keep_rare_words):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        docid, line = line.split('\\t', 1)\n",
    "        url, text = line.split('\\t', 1)\n",
    "\n",
    "        if keyword in docid:\n",
    "            docid = docid.replace('\\t', '')\n",
    "            line = self.preprocess_line(text, keep_rare_words)\n",
    "            data.append((docid, line))\n",
    "    return data\n",
    "\n",
    "def preprocess_line(self, line, keep_rare_words = True, use_stemming = True):\n",
    "    line = self.remove_punctuation(line)\n",
    "    words = line.split()\n",
    "    words_to_keep = []\n",
    "    for word in words:\n",
    "        word = word.lower().strip()\n",
    "        if word not in self.stop_words:\n",
    "            if not self.has_more_digits(word):\n",
    "                if use_stemming:\n",
    "                   word = self.ps.stem(word)\n",
    "                if keep_rare_words:\n",
    "                    words_to_keep.append(word)\n",
    "                else:\n",
    "                    if word not in self.rare_words:\n",
    "                        words_to_keep.append(word)\n",
    "\n",
    "        new_line = ' '.join(words_to_keep)\n",
    "\n",
    "    return self.clean_digits(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_digits(self, line):\n",
    "    words_to_keep = []\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        count_digits = 0\n",
    "        count_chars = 0\n",
    "        for w in word:\n",
    "            if w.isdigit():\n",
    "                count_digits = count_digits + 1\n",
    "            elif w == '-':\n",
    "                count_digits = count_digits + 1\n",
    "            elif w == '/':\n",
    "                count_digits = count_digits + 1\n",
    "            else:\n",
    "                count_chars = count_chars + 1\n",
    "        if count_chars > count_digits:\n",
    "            words_to_keep.append(word)\n",
    "    return ' '.join(words_to_keep)\n",
    "\n",
    "\n",
    "def remove_punctuation(self, txt):\n",
    "    txt = txt.replace('-', ' ')\n",
    "    txt = txt.replace('/', ' ')\n",
    "    return re.sub('[^A-Za-z0-9\\s]+', '', txt)\n",
    "\n",
    "def has_more_digits(self, txt):\n",
    "    count = 0\n",
    "    for ch in txt:\n",
    "        if ch.isdigit():\n",
    "            count = count + 1\n",
    "    if count > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy\n",
    "from random import shuffle\n",
    "\n",
    "import timeit\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "import timeit\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_words(input_file_names):\n",
    "    sentence_words=[]\n",
    "    for file_name in input_file_names:\n",
    "        print(file_name)\n",
    "        for line in open(file_name, encoding=\"utf8\"):\n",
    "            line=line.strip().lower()\n",
    "            line=get_words(line)\n",
    "            sent_words=tokenize(line)\n",
    "            if len(sent_words) >1:\n",
    "                sentence_words.append(sent_words)\n",
    "    return sentence_words\n",
    "\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?',sent) if x.strip()]\n",
    "\n",
    "def get_words(line):\n",
    "        line = ' '.join([word for word in line.split() if word not in cachedStopWords])\n",
    "        line = ' '.join([re.sub(r'[^\\w\\s]','',word) for word in line.split() if word not in cachedStopWords])\n",
    "        line = ' '.join([word for word in line.split() if not word.isdigit()])\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = parse_sentence_words(input_file_names) # should be an array of arrays, where each inner array is the sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        sentences,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive='search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_word2vec.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTrainer():\n",
    "\n",
    "    def __init__(self, file_to_data):\n",
    "        self.df_corpus = pd.read_csv(file_to_data, delimiter='\\t')\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.docLabels = []\n",
    "        self.data = []\n",
    "\n",
    "        for index, row in self.df_corpus.iterrows():\n",
    "            self.docLabels.append(row['id'])\n",
    "            prep_doc = row['text']\n",
    "            self.data.append(prep_doc)\n",
    "\n",
    "    def init_model(self, vec_size, window):\n",
    "        self.it = self.LabeledLineSentence(self.data, self.docLabels)\n",
    "        self.model = gensim.models.Doc2Vec(vector_size=vec_size, window=window, min_count=5, workers=mp.cpu_count(), alpha=0.025,\n",
    "                                      min_alpha=0.025)  # use fixed learning rate\n",
    "        self.model.build_vocab(self.it.to_array())\n",
    "\n",
    "    def train_model(self, number_epochs):\n",
    "        self.model.train(self.it, epochs=number_epochs, total_examples=self.model.corpus_count)\n",
    "        self.model.save('./doc2vec_model.d2v')\n",
    "        return self.model\n",
    "\n",
    "    class LabeledLineSentence(object):\n",
    "        def __init__(self, doc_list, labels_list):\n",
    "            self.labels_list = labels_list\n",
    "            self.doc_list = doc_list\n",
    "\n",
    "        def __iter__(self):\n",
    "            for idx, doc in enumerate(self.doc_list):\n",
    "                yield TaggedDocument(doc.split(), [self.labels_list[idx]])\n",
    "\n",
    "        def to_array(self):\n",
    "            self.sentences = []\n",
    "            for idx, doc in enumerate(self.doc_list):\n",
    "                self.sentences.append(TaggedDocument(doc.split(), [self.labels_list[idx]]))\n",
    "            return self.sentences\n",
    "\n",
    "        def sentences_perm(self):\n",
    "            shuffle(self.sentences)\n",
    "            return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecSearch():\n",
    "\n",
    "    def __init__(self, path_to_model, file_to_dict):\n",
    "        self.model = Doc2Vec.load(path_to_model)\n",
    "        #with open(file_to_dict, 'rb') as handle:\n",
    "         #   self.docs_dict = pickle.load(handle)\n",
    "\n",
    "    def get_most_similar_terms(self, term, k):\n",
    "        return self.model.wv.most_similar(term, topn=k)\n",
    "\n",
    "    def get_results_as_df(self, sim_docs):\n",
    "        results_df = pd.DataFrame(columns=['doc_id', 'confidence', 'userurl', 'keywords'])\n",
    "\n",
    "        for doc in sim_docs:\n",
    "            doc_id = doc[0]\n",
    "            entry = self.docs_dict.get(doc_id)[0]\n",
    "            results_df = results_df.append(\n",
    "                {'doc_id': doc_id, 'confidence': doc[1], 'userurl': entry[0], 'keywords': entry[1]}, ignore_index=True)\n",
    "        return results_df\n",
    "\n",
    "    def get_results_as_array(self, sim_docs):\n",
    "        results = []\n",
    "        for doc in sim_docs:\n",
    "            doc_id = doc[0]\n",
    "            entry = self.docs_dict.get(doc_id)[0]\n",
    "            results.append((doc_id, doc[1], entry[0], entry[1]))\n",
    "        return results\n",
    "\n",
    "    def search(self, query, k):\n",
    "        query_vec = self.model.infer_vector(query.split())\n",
    "        sim_docs = self.model.docvecs.most_similar([query_vec], topn=k)\n",
    "        return sim_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering, ref. https://www.kaggle.com/sgunjan05/document-clustering-using-doc2vec-word2vec/code\n",
    "kmeans_model = KMeans(n_clusters=4, init='k-means++', max_iter=100)  \n",
    "X = kmeans_model.fit(saved_model.docvecs.vectors_docs)\n",
    "labels=kmeans_model.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = kmeans_model.fit_predict(saved_model.docvecs.vectors_docs)\n",
    "pca = PCA(n_components=2).fit(saved_model.docvecs.vectors_docs)\n",
    "datapoint = pca.transform(saved_model.docvecs.vectors_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans_model.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s = 150, c='#000000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_cluster(labels, docs):\n",
    "    topics_clusters = defaultdict(list)\n",
    "    \n",
    "    if(len(labels)!= len(docs)):\n",
    "        print ('Number of labels must be equal to the number of documents.')\n",
    "        \n",
    "    else:    \n",
    "        for i in range(len(labels)):\n",
    "            topics_clusters[labels[i]].append(docs[i])\n",
    "    \n",
    "    return topics_clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_distribution(clusters):\n",
    "    total = 0\n",
    "    for key, value in clusters.items():\n",
    "        c_docs_count = len(clusters[key])\n",
    "        print('Cluster ', str(key), ' ', c_docs_count)\n",
    "        total += c_docs_count\n",
    "    print ('Total docs count ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = assign_to_cluster(labels, data_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequen_words(clusters_dict)\n",
    "    for key, value in clusters_dict.items():\n",
    "        print('Cluster '+ str(key))\n",
    "        keywords = get_most_common(value)\n",
    "        print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "\n",
    "    def __init__(self, path_to_gs):\n",
    "        self.gold_standard_dict = pickle.load(open(path_to_gs, \"rb\"))\n",
    "\n",
    "\n",
    "    def precision_at_k_evaluation(self, query_id, predicted_docs, k):\n",
    "        top_k_docs = list()\n",
    "\n",
    "        for doc_id in range(k[0]):\n",
    "            top_k_docs.append(predicted_docs[doc_id])\n",
    "\n",
    "        measure = self.precision_evaluation(query_id, top_k_docs)\n",
    "\n",
    "        return measure\n",
    "\n",
    "    def ape_evaluation(self, query_id, predicted_docs):\n",
    "        sum_precisions = 0\n",
    "        count_correct_predictions = 0\n",
    "        \n",
    "        for doc_id in predicted_docs:\n",
    "            if doc_id in self.gold_standard_dict[query_id]:\n",
    "                count_correct_predictions += 1\n",
    "                rank = np.where(predicted_docs == doc_id)[0] + 1 \n",
    "                sumPrecisions += count_correct_predictions / rank[0] \n",
    "                \n",
    "        if count_correct_predictions == 0:\n",
    "            measure = 0\n",
    "        else:\n",
    "            measure = sum_precisions / count_correct_predictions\n",
    "        return measure\n",
    "\n",
    "    '''nDCG idea: input is the ranked docs for the query:\n",
    "     - for each doc, lookup whether it is in the assigned as releveant docs to this query, \n",
    "     - if yes- take its relevancy score, compute the discount factor based on the rank'''\n",
    "\n",
    "    def compute_dcg_one_query(self, query_id, predicted_docs, best=False):\n",
    "        i = 1\n",
    "        dcg_accumulated = 0\n",
    "        if not best:\n",
    "            for doc_id in predicted_docs:\n",
    "                disc_factor = 1 / np.log2(max(i, 2))\n",
    "                relevancy = self.get_true_relevancy(query_id, doc_id)\n",
    "                gain = np.multiply(disc_factor, relevancy)\n",
    "                dcg_accumulated = dcg_accumulated + gain\n",
    "                # print(dcg_accumulated)\n",
    "                i = i + 1\n",
    "        else:\n",
    "            for doc_id, score in predicted_docs:\n",
    "                disc_factor = 1 / np.log2(max(i, 2))\n",
    "                relevancy = float(score)\n",
    "                gain = np.multiply(disc_factor, relevancy)\n",
    "                dcg_accumulated = dcg_accumulated + gain\n",
    "                # print(dcg_accumulated)\n",
    "                i = i + 1\n",
    "\n",
    "        return float(\"{0:.2f}\".format(dcg_accumulated))\n",
    "\n",
    "    def get_true_relevancy(self, query_id, doc_id):\n",
    "        try:\n",
    "            score = self.gold_standard_dict[query_id][doc_id]\n",
    "        except KeyError as err:\n",
    "            score = 0\n",
    "        return float(score)\n",
    "\n",
    "    def compute_best_dcg_one_query(self, query_id):\n",
    "\n",
    "        true_docs = self.gold_standard_dict[query_id]\n",
    "        true_docs_best = sorted(true_docs.items(), key = operator.itemgetter(1), reverse = True)\n",
    "        best_dcg = self.compute_dcg_one_query(query_id, true_docs_best, True)\n",
    "\n",
    "        return best_dcg\n",
    "\n",
    "    def ndcg_evaluation(self, query_id, predicted_docs):\n",
    "        dcg = self.compute_dcg_one_query(query_id, predicted_docs)\n",
    "        best_dcg = self.compute_best_dcg_one_query(query_id)\n",
    "\n",
    "        if best_dcg == 0:\n",
    "            best_dcg = 1\n",
    "\n",
    "        return dcg / best_dcg\n",
    "\n",
    "    def evaluate_map_and_ndcg(self, queries, search, nr_docs):\n",
    "        average_precision_GLOBAL = 0\n",
    "        count=0\n",
    "        ndcg_GLOBAL = 0\n",
    "        time_GLOBAL = 0\n",
    "   \n",
    "        for index, row in queries.iterrows():\n",
    "            q_id= row['id'] # id to lookup in the gold standard\n",
    "            query_text= row['query'] # query to test the retrieval model against\n",
    "            start_time=time.time()\n",
    "            predicted_docs= search.retrieve(query_text, nr_docs)\n",
    "            elapsed_time= time.time()-start_time\n",
    "            time_GLOBAL += elapsed_time\n",
    "            average_precision_GLOBAL += self.ape_evaluation(q_id, predicted_docs)\n",
    "            ndcg_GLOBAL += self.ndcg_evaluation(q_id, predicted_docs)\n",
    "            count+=1\n",
    "            if count % 500 == 0:\n",
    "                print('Processed doc: ', count)  \n",
    "        mean_ap = average_precision_GLOBAL/count\n",
    "        mean_ndcg = ndcg_GLOBAL/count\n",
    "        avg_time = time_GLOBAL/count\n",
    "    \n",
    "        print(\"Mean Average Precision = {}\".format(mean_ap))\n",
    "        print(\"NDCG = {}\".format(mean_ndcg ))\n",
    "        print(\"Time total= {}\".format(time_GLOBAL))\n",
    "        print(\"Time AVG= {}\".format(avg_time))\n",
    "    \n",
    "        return mean_ap, mean_ndcg, elapsed_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
