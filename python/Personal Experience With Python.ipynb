{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with specific column names\n",
    "corpus_df = pd.DataFrame(columns=['id', 'userurl', 'source', 'title', 'description', 'content', 'keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append one row, fill by column name\n",
    "corpus_df = corpus_df.append(\n",
    "                    {'id': 0, 'userurl': 'url', 'source': 'source', 'title': 'title',\n",
    "                     'description': 'description', 'content': 'content',\n",
    "                     'keywords': 'keywords'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new column, being the concatenation of other columns\n",
    "corpus_df[\"text\"] = corpus_df[\"title\"].map(str) + ' ' + corpus_df[\"description\"] + ' ' + corpus_df[\n",
    "            \"content\"].map(str) + ' ' + corpus_df[\"keywords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "corpus_df = corpus_df.drop(['source', 'userurl', 'title', 'description', 'keywords', 'content'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dfs: https://stackoverflow.com/questions/32444138/combine-a-list-of-pandas-dataframes-to-one-pandas-dataframe\n",
    "df = pd.concat(list_of_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing/ Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write and read csv\n",
    "file_name = 'test.csv'\n",
    "corpus_df.to_csv(file_name, sep='\\t')\n",
    "# diff encoding\n",
    "corpus_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "# without index values\n",
    "corpus_df.to_csv(file_name, encoding='utf-8', index=False)\n",
    "# append\n",
    "with open(file_name, 'a') as f:\n",
    "    df.to_csv(f, header=False)\n",
    "# load\n",
    "df_corpus = pd.read_csv(file_name, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging \n",
    "comparison = pd.merge(results_sap, results_doc2vec, how='inner', on=['doc_id']) # if on is not specified, it is done on the index?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/4530611/saving-and-loading-objects-and-using-pickle\n",
    "with open(r\"someobject.pickle\", \"wb\") as output_file:\n",
    "    cPickle.dump(d, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"someobject.pickle\", \"rb\") as input_file:\n",
    "    e = cPickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/3108042/get-max-key-in-\n",
    "max(dict_m, key=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get value by key, where the values are tuples!\n",
    "def get_url_key(url_to_find):\n",
    "    url_key = -1\n",
    "    for key, value in docs_dict.items():\n",
    "        if value[0][0] == url_to_find:\n",
    "            url_key = key\n",
    "    return url_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/32957708/python-pickle-error-unicodedecodeerror\n",
    "#https://stackoverflow.com/questions/9415785/merging-several-python-dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networ Communication\n",
    "- Interacting with the outside world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "!{sys.executable} -m pip install pyhdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pyhdb.connect(host=\"<mo-6770....>\", port=<port>, user=\"<user>\", password=\"<pass>\") # dummy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample query construction\n",
    "query = ''' SELECT \"USERURL\",\"TITLE\", \"CONTENT\", \"DESCRIPTION\", \"SOURCE\",\"KEYWORDS\"\n",
    "                    FROM REPO_.\"T.CONTENT\"   \n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many rows match the query\n",
    "N = cursor.execute(\"SELECT COUNT(*) FROM (\" + query + \")\").fetchone()[0]\n",
    "        print('Fetching ', N, ' documents...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work row by row\n",
    "for i in range(N):\n",
    "    try:\n",
    "        row = cursor.fetchone()\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print('Processing document ', i)\n",
    "        if row[0] is not None:\n",
    "            userurl = row[0]\n",
    "        else:\n",
    "            userurl = \"\"\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    except UnicodeEncodeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all rows\n",
    "results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://thepythonguru.com/fetching-records-using-fetchone-and-fetchmany/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'client'\n",
    "passw = 'dummypass'\n",
    "url = 'https://<search>.com/api/v1/search'\n",
    "headers = {'Content-Type': 'application/json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = get_api_body(query)\n",
    "resp = requests.post(url, data=request_body, auth=(user, passw), headers=headers)\n",
    "resp_json = resp.json()\n",
    "results = resp_json['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_body(self, query):\n",
    "    data = ''' {\n",
    "            \"repository\": \"srh\",\n",
    "            \"type\": \"content\",\n",
    "            \"filters\": [{\n",
    "                    \"field\": \"CONTENT\",\n",
    "                    \"type\": \"fuzzy\",\n",
    "                    \"values\": [\"''' + query + '''\"],\n",
    "                    \"fuzzy\": {\n",
    "                        \"level\": 0.9,\n",
    "                        \"weight\": 0.2,\n",
    "                        \"parameters\": \"def\"\"\n",
    "                    },\n",
    "                    \"logicalOperator\": \"or\"\n",
    "                },\n",
    "                {\n",
    "                    \"field\": \"TITLE\",\n",
    "                    \"type\": \"fuzzy\",\n",
    "                    \"values\": [\"''' + query + '''\"],\n",
    "                    \"fuzzy\": {\n",
    "                        \"level\": 0.9,\n",
    "                        \"weight\": 1,\n",
    "                        \"parameters\": \"def\"\"\n",
    "                    }\n",
    "                }\n",
    "               '''\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results being an array of separate JSON objects\n",
    "def get_results_as_df(self, results):\n",
    "    results_df = pd.DataFrame(columns=['doc_id', 'userurl', 'title'])\n",
    "    # results[i] to access each subsequent/ separate JSON element\n",
    "    for i in range(len(results)):\n",
    "        results_df = results_df.append(\n",
    "            {'doc_id': doc_id, 'userurl': results[i]['USERURL'], 'title': results[i]['TITLE'],\n",
    "             'keywords': results[i]['KEYWORDS']}, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Utilization\n",
    "- usefull to check effects is htop, e.g. see https://peteris.rocks/blog/htop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results being a list of tuple(any) elements\n",
    "def get_pool_data(results):\n",
    "    \n",
    "    pool = mp.Pool()\n",
    "    deserialized_results_list = list(map(deserialize, results))\n",
    "    \n",
    "    results_mp = pool.map(preprocess_row, deserialized_results_list)   \n",
    "    df_global = pd.concat(results_mp)\n",
    "    \n",
    "    return df_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Deployment with Flask\n",
    "- small search & table view example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from os.path import dirname, realpath\n",
    "from doc2vec_search import Doc2VecSearch\n",
    "from time import time\n",
    "from flask import Flask, Response, request\n",
    "\n",
    "PATH_TO_MODEL = \"data/models/model_dbow_100_10_no_ppl.d2v\"\n",
    "PATH_TO_DICT = \"data/dict/docs_dict.pickle\"\n",
    "\n",
    "if 'is_docker' in os.environ:\n",
    "    CWD = \"/app/data\"\n",
    "else:\n",
    "    CWD = dirname(dirname(realpath(__file__))) + \"/data\"\n",
    "\n",
    "app = Flask(__name__, template_folder=dirname(realpath(__file__)))\n",
    "\n",
    "doc2vec = Doc2VecSearch(PATH_TO_MODEL, PATH_TO_DICT)\n",
    "url = \"/unique/search\"\n",
    "\n",
    "@app.route(url, methods=['GET'])\n",
    "def doc2vec_search():\n",
    "    q = request.args.get('q')\n",
    "\n",
    "    exec_time = 0\n",
    "    response = dict()\n",
    "\n",
    "    if q:\n",
    "        q = q.lower()\n",
    "        start = time()\n",
    "        results = doc2vec.search(q, 10)\n",
    "        exec_time = int((time() - start) * 1000)\n",
    "\n",
    "        if results:\n",
    "            for i in range(len(results)):\n",
    "                response['data'+str(i)] = [{'userurl': results[i][0], 'title': results[i][1], 'keywords': results[i][2],\n",
    "                                   'source': results[i][3]}]\n",
    "                response['metadata'+str(i)] = {'executionTime': exec_time, 'status': 200, 'itemCount': 1}\n",
    "            return Response(json.dumps(response, indent=2), status=200, mimetype='application/json')\n",
    "\n",
    "    response['data'] = []\n",
    "    response['metadata'] = {'executionTime': exec_time, 'status': 200, 'itemCount': 0}\n",
    "    return Response(json.dumps(response, indent=2), status=200, mimetype='application/json')\n",
    "\n",
    "\n",
    "@app.route('/healthcheck')\n",
    "def healthckeck():\n",
    "    return \"All is well\"\n",
    "\n",
    "@app.route('/' + 'pointer/testing')\n",
    "def testing():\n",
    "    page = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\">\n",
    "    </head>\n",
    "        <body>\n",
    "            <form>\n",
    "                <input name=\"query\" autofocus>\n",
    "                <input type=\"submit\">\n",
    "                Number of results to show:\n",
    "                <input name=\"n\">\n",
    "            </form>\n",
    "            <br>\n",
    "    \"\"\"\n",
    "\n",
    "    q = request.args.get('query')\n",
    "    n = request.args.get('n')\n",
    "\n",
    "    if q:\n",
    "        q = q.lower().strip()\n",
    "        page += \"Current query: <b><i>\" + q + \"</i></b><br>\\n\"\n",
    "\n",
    "        if n:\n",
    "            n = int(n)\n",
    "            results = doc2vec.search(q, n)\n",
    "        else:\n",
    "            results = doc2vec.search(q, 10)\n",
    "\n",
    "        page += \"<h2>Doc2Vec Search Results</h2>\"\n",
    "        page += \"\"\"<table class=\"w3-table-all\"><tr><th></th><th>Userurl</th><th>Title</th><th>Keywords</th><th>Source</th><th>Similarity Score</th></tr>\"\"\"\n",
    "\n",
    "        if results:\n",
    "            for i in range(len(results)):\n",
    "                page += \"<tr><td>\" + str(i) + \"</td><td>\" + results[i][0] + \"</td><td>\" + results[i][1] + \"</td><td>\" + results[i][2] + \"</td><td>\" + results[i][3]+ \"</td><td>\" + \"{0:.2f}\".format(results[i][4])  + \"</td><tr>\"\n",
    "        page += \"</table>\"\n",
    "    return page + \"</body></html>\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This is only called when starting file directly. Not in Docker container.\n",
    "    logger.info(\"Api is ready. Try: http://localhost:5021/test/doc2vec?q=mster%20data%20mannagemnt\")\n",
    "    app.run(port=5021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/01.05-ipython-and-shell-commands.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    if i < 136000:\n",
    "        print('skiping ',i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "count = 1\n",
    "# Read in all files from the current directory, that match the prefix. \n",
    "for file in glob.glob(\"archive_sitemap_*\"):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53513/how-do-i-check-if-a-list-is-empty\n",
    "if not a:\n",
    "    print(\"List is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen and Sessions\n",
    "# https://stackoverflow.com/questions/1509677/kill-detached-screen-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote connections\n",
    "#https://superuser.com/questions/23911/running-commands-on-putty-without-fear-of-losing-connection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
