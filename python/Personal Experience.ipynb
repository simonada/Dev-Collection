{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with specific column names\n",
    "corpus_df = pd.DataFrame(columns=['id', 'userurl', 'source', 'title', 'description', 'content', 'keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append one row, fill by column name\n",
    "corpus_df = corpus_df.append(\n",
    "                    {'id': 0, 'userurl': 'url', 'source': 'source', 'title': 'title',\n",
    "                     'description': 'description', 'content': 'content',\n",
    "                     'keywords': 'keywords'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new column, being the concatenation of other columns\n",
    "corpus_df[\"text\"] = corpus_df[\"title\"].map(str) + ' ' + corpus_df[\"description\"] + ' ' + corpus_df[\n",
    "            \"content\"].map(str) + ' ' + corpus_df[\"keywords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "corpus_df = corpus_df.drop(['source', 'userurl', 'title', 'description', 'keywords', 'content'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing/ Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write and read csv\n",
    "file_name = 'test.csv'\n",
    "corpus_df.to_csv(file_name, sep='\\t')\n",
    "# diff encoding\n",
    "corpus_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "# without index values\n",
    "corpus_df.to_csv(file_name, encoding='utf-8', index=False)\n",
    "# load\n",
    "df_corpus = pd.read_csv(file_name, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging \n",
    "comparison = pd.merge(results_sap, results_doc2vec, how='inner', on=['doc_id']) # if on is not specified, it is done on the index?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get value by key, where the values are tuples!\n",
    "def get_url_key(url_to_find):\n",
    "    url_key = -1\n",
    "    for key, value in docs_dict.items():\n",
    "        if value[0][0] == url_to_find:\n",
    "            url_key = key\n",
    "    return url_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networ Communication\n",
    "- Interacting with the outside world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "!{sys.executable} -m pip install pyhdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pyhdb.connect(host=\"<mo-6770....>\", port=<port>, user=\"<user>\", password=\"<pass>\") # dummy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample query construction\n",
    "query = ''' SELECT \"USERURL\",\"TITLE\", \"CONTENT\", \"DESCRIPTION\", \"SOURCE\",\"KEYWORDS\"\n",
    "                    FROM REPO_.\"T.CONTENT\"   \n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many rows match the query\n",
    "N = cursor.execute(\"SELECT COUNT(*) FROM (\" + query + \")\").fetchone()[0]\n",
    "        print('Fetching ', N, ' documents...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work row by row\n",
    "for i in range(N):\n",
    "    try:\n",
    "        row = cursor.fetchone()\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print('Processing document ', i)\n",
    "        if row[0] is not None:\n",
    "            userurl = row[0]\n",
    "        else:\n",
    "            userurl = \"\"\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    except UnicodeEncodeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'client'\n",
    "passw = 'dummypass'\n",
    "url = 'https://onedx.find.sap.com/api/v1/search'\n",
    "headers = {'Content-Type': 'application/json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = get_api_body(query)\n",
    "resp = requests.post(url, data=request_body, auth=(user, passw), headers=headers)\n",
    "resp_json = resp.json()\n",
    "results = resp_json['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_body(self, query):\n",
    "    data = ''' {\n",
    "            \"repository\": \"srh\",\n",
    "            \"type\": \"content\",\n",
    "            \"filters\": [{\n",
    "                    \"field\": \"CONTENT\",\n",
    "                    \"type\": \"fuzzy\",\n",
    "                    \"values\": [\"''' + query + '''\"],\n",
    "                    \"fuzzy\": {\n",
    "                        \"level\": 0.9,\n",
    "                        \"weight\": 0.2,\n",
    "                        \"parameters\": \"def\"\"\n",
    "                    },\n",
    "                    \"logicalOperator\": \"or\"\n",
    "                },\n",
    "                {\n",
    "                    \"field\": \"TITLE\",\n",
    "                    \"type\": \"fuzzy\",\n",
    "                    \"values\": [\"''' + query + '''\"],\n",
    "                    \"fuzzy\": {\n",
    "                        \"level\": 0.9,\n",
    "                        \"weight\": 1,\n",
    "                        \"parameters\": \"def\"\"\n",
    "                    }\n",
    "                }\n",
    "               '''\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results being an array of separate JSON objects\n",
    "def get_results_as_df(self, results):\n",
    "    results_df = pd.DataFrame(columns=['doc_id', 'userurl', 'title'])\n",
    "    # results[i] to access each subsequent/ separate JSON element\n",
    "    for i in range(len(results)):\n",
    "        results_df = results_df.append(\n",
    "            {'doc_id': doc_id, 'userurl': results[i]['USERURL'], 'title': results[i]['TITLE'],\n",
    "             'keywords': results[i]['KEYWORDS']}, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(self, doc):\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    content = re.sub('[^A-Za-z]+', ' ', doc)\n",
    "    content = content.lower().split()\n",
    "    content = ' '.join([word for word in content if word not in stop_words and len(word) > 1])\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/01.05-ipython-and-shell-commands.html\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
